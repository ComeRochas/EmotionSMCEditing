{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b714eb8b",
   "metadata": {},
   "source": [
    "# Latent Diffusion: Training & Inference on CelebA-HQ\n",
    "\n",
    "This notebook implements the training of a Latent Diffusion Model (LDM) on the CelebA-HQ dataset, followed by Sequential Monte Carlo (SMC) inference for emotion editing.\n",
    "\n",
    "### Prerequisites\n",
    "The dataset must be added to the Kaggle environment (e.g., `badasstechie/celebahq-resized-256x256`).\n",
    "The expected path is `/kaggle/input/celebahq-resized-256x256`.\n",
    "\n",
    "You should also add your unet trained model to the Kaggle environment.\n",
    "If you want to track training progress using WandB, you can add your WANDB_API_KEY to Kaggle Secrets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c898d6",
   "metadata": {},
   "source": [
    "## 1. Environment and Configuration Setup\n",
    "\n",
    "###  Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers accelerate transformers wandb\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import AutoencoderKL, UNet2DModel, DDPMScheduler\n",
    "from accelerate import Accelerator\n",
    "import wandb\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"Libraries installed and imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207f6fe",
   "metadata": {},
   "source": [
    "### Global Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e6313",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "TRAIN_CONFIG = {\n",
    "    \"data_dir\": \"/kaggle/input/celebahq-resized-256x256\",\n",
    "    \"output_dir\": \"checkpoints\",\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 100, \n",
    "    \"lr\": 1e-4,\n",
    "    \"mixed_precision\": \"fp16\",\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"num_train_timesteps\": 1000,\n",
    "    \"beta_schedule\": \"scaled_linear\",\n",
    "    \"prediction_type\": \"epsilon\",\n",
    "    \"checkpoint_path\": None,  # For continuing training from a checkpoint\n",
    "    \"start_epoch\": 0  # You may specify the starting epoch if resuming\n",
    "}\n",
    "\n",
    "RUN_TRAINING = False # For skipping the training loop\n",
    "RUN_BENCHMARKING = False # For skipping the benchmarking loop\n",
    "\n",
    "TEST_CONFIG = {\n",
    "    \"data_dir\": \"/kaggle/input/celebahq-resized-256x256\", \n",
    "    \"output_dir\": \"/results\",\n",
    "    \"trained_unet_path\": \"checkpoint-epoch-89\",\n",
    "    \"test_image_path\": \"testimage.jpg\"\n",
    "}\n",
    "\n",
    "print(f\"Device used: {DEVICE}\")\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f986285",
   "metadata": {},
   "source": [
    "###  Experiment Tracking with Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f308c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic connection to WandB via Kaggle Secrets\n",
    "load_dotenv() \n",
    "\n",
    "if \"WANDB_API_KEY\" in os.environ:\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "    print(\"Connected to WandB via Environment Variable.\")\n",
    "else:\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "        wandb.login(key=wandb_key)\n",
    "        print(\"Connected to WandB via Kaggle Secrets.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to automatically connect to WandB: {e}\")\n",
    "        print(\"Please ensure 'WANDB_API_KEY' is added in Add-ons -> Secrets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23a052",
   "metadata": {},
   "source": [
    "## 2. Loading datasets and models\n",
    "\n",
    "### Dataset Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebAHQDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Recursive search for images\n",
    "        self.image_paths = glob.glob(os.path.join(root_dir, \"**\", \"*.jpg\"), recursive=True) + \\\n",
    "                           glob.glob(os.path.join(root_dir, \"**\", \"*.png\"), recursive=True)\n",
    "        \n",
    "        print(f\"Found {len(self.image_paths)} images in {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image\n",
    "\n",
    "def get_transforms(image_size=256):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize RGB (3 channels) to [-1, 1]\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5917536f",
   "metadata": {},
   "source": [
    "### Model Loading: VAE, UNet & Emotion Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f85f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vae(model_id=\"runwayml/stable-diffusion-v1-5\"):\n",
    "    print(f\"Loading VAE from {model_id}...\")\n",
    "    vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\")\n",
    "    vae.eval()\n",
    "    vae.requires_grad_(False) # Freeze VAE\n",
    "    return vae\n",
    "\n",
    "def get_unet(image_size=32, in_channels=4, out_channels=4):\n",
    "    # image_size here is the latent size (256 / 8 = 32)\n",
    "    print(f\"Creating UNet2DModel with input/output channels={in_channels}, latent_size={image_size}...\")\n",
    "    unet = UNet2DModel(\n",
    "        sample_size=image_size,\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        layers_per_block=2,\n",
    "        block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Calculate and display number of parameters\n",
    "    num_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "    print(f\"--> UNet model initialized with {num_params:,} parameters.\")\n",
    "    \n",
    "    return unet\n",
    "\n",
    "def get_emotion_classifier(model_id=\"dima806/facial_emotions_image_detection\"):\n",
    "    print(f\"Loading Emotion Classifier from {model_id}...\")\n",
    "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "    model = AutoModelForImageClassification.from_pretrained(model_id)\n",
    "    model.eval()\n",
    "    model.requires_grad_(False)\n",
    "    return processor, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7883a8",
   "metadata": {},
   "source": [
    "## 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f85f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_inference(unet, num_samples=4, save_path=None):\n",
    "    device = DEVICE\n",
    "    \n",
    "    # If unet is wrapped by accelerator, use it directly or unwrap\n",
    "    if isinstance(unet, torch.nn.parallel.DistributedDataParallel):\n",
    "        unet = unet.module\n",
    "    unet.to(device)\n",
    "    unet.eval()\n",
    "    \n",
    "    vae = get_vae().to(device)\n",
    "    \n",
    "    # Scheduler for inference (Important : it must be the same config as train)\n",
    "    scheduler = DDPMScheduler(\n",
    "        num_train_timesteps=TRAIN_CONFIG[\"num_train_timesteps\"],\n",
    "        beta_schedule=TRAIN_CONFIG[\"beta_schedule\"],\n",
    "        prediction_type=TRAIN_CONFIG[\"prediction_type\"],\n",
    "        clip_sample=False\n",
    "    )\n",
    "    scheduler.set_timesteps(TRAIN_CONFIG[\"num_train_timesteps\"])\n",
    "    \n",
    "    # 1. Generate initial noise (Latents)\n",
    "    latents = torch.randn((num_samples, 4, 32, 32), device=device)\n",
    "    \n",
    "    # Indices to capture progress (0%, 25%, 50%, 75%, 100%)\n",
    "    total_steps = TRAIN_CONFIG[\"num_train_timesteps\"]\n",
    "    capture_indices = [0, total_steps // 4, total_steps // 2, 3 * total_steps // 4] \n",
    "    captured_images = []\n",
    "    \n",
    "    print(\"Starting inference...\")\n",
    "    for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
    "        # Intermediate capture\n",
    "        if i in capture_indices:\n",
    "            with torch.no_grad():\n",
    "                # Quick decode for visualization\n",
    "                temp_latents = latents / vae.config.scaling_factor\n",
    "                decoded = vae.decode(temp_latents).sample\n",
    "                decoded = (decoded / 2 + 0.5).clamp(0, 1)\n",
    "                decoded = decoded.cpu().permute(0, 2, 3, 1).numpy()\n",
    "                captured_images.append(decoded)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Predict noise\n",
    "            noise_pred = unet(latents, t).sample\n",
    "            \n",
    "            # Scheduler step (remove noise)\n",
    "            latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "            \n",
    "    # Final capture (100%)\n",
    "    with torch.no_grad():\n",
    "        latents = latents / vae.config.scaling_factor\n",
    "        images = vae.decode(latents).sample\n",
    "        images = (images / 2 + 0.5).clamp(0, 1)\n",
    "        images = images.cpu().permute(0, 2, 3, 1).numpy()\n",
    "        captured_images.append(images)\n",
    "        \n",
    "    # 3. Display: Grid (num_samples x 5)\n",
    "    num_steps = len(captured_images)\n",
    "    fig, axes = plt.subplots(num_samples, num_steps, figsize=(3 * num_steps, 3 * num_samples))\n",
    "    \n",
    "    # Handle case num_samples=1\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "        \n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_steps):\n",
    "            axes[i, j].imshow(captured_images[j][i])\n",
    "            axes[i, j].axis(\"off\")\n",
    "            if i == 0:\n",
    "                step_label = f\"Step {capture_indices[j]}\" if j < len(capture_indices) else \"Final\"\n",
    "                axes[i, j].set_title(step_label)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved visualization to {save_path}\")\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "# Training Loop        \n",
    "def train_loop():\n",
    "    # Use global configuration\n",
    "    config = TRAIN_CONFIG.copy()\n",
    "    \n",
    "    # Check dataset path (fallback if local for test)\n",
    "    if not os.path.exists(config[\"data_dir\"]):\n",
    "        print(f\"Warning: {config['data_dir']} not found. Checking local ./data...\")\n",
    "        if os.path.exists(\"./data\"):\n",
    "            config[\"data_dir\"] = \"./data\"\n",
    "        else:\n",
    "            print(\"Error: Dataset not found.\")\n",
    "            return\n",
    "\n",
    "    # Initialize Accelerator with WandB\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config[\"mixed_precision\"],\n",
    "        gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "        log_with=\"wandb\"\n",
    "    )\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "        # Initialize WandB project\n",
    "        accelerator.init_trackers(\"celebahq-latent-diffusion\", config=config)\n",
    "\n",
    "    # Load Models\n",
    "    vae = get_vae()\n",
    "    \n",
    "    resume_from_checkpoint = config[\"checkpoint_path\"] \n",
    "    start_epoch = config[\"start_epoch\"] if resume_from_checkpoint else 0\n",
    "    if resume_from_checkpoint:\n",
    "        print(f\"Resuming training from: {resume_from_checkpoint}\")\n",
    "        # Load pre-trained UNet\n",
    "        unet = UNet2DModel.from_pretrained(resume_from_checkpoint)\n",
    "    else:\n",
    "        unet = get_unet(image_size=32)\n",
    "    \n",
    "    # Scheduler Configuration \n",
    "    noise_scheduler = DDPMScheduler(\n",
    "        num_train_timesteps=config[\"num_train_timesteps\"],\n",
    "        beta_schedule=config[\"beta_schedule\"],\n",
    "        prediction_type=config[\"prediction_type\"],\n",
    "        clip_sample=False # Important for latent diffusion\n",
    "    )\n",
    "\n",
    "    # Freeze VAE and move to device\n",
    "    vae.to(accelerator.device)\n",
    "    vae.eval()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(unet.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # Dataset & Dataloader\n",
    "    dataset = CelebAHQDataset(root_dir=config[\"data_dir\"], transform=get_transforms(256))\n",
    "    \n",
    "    # IMPORTANT: num_workers=0 to avoid blocks on macOS or some notebooks\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=0, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Prepare\n",
    "    unet, optimizer, train_dataloader = accelerator.prepare(\n",
    "        unet, optimizer, train_dataloader\n",
    "    )\n",
    "\n",
    "    # Training Loop\n",
    "    global_step = 0\n",
    "    \n",
    "    print(f\"Starting training from epoch {start_epoch} to {start_epoch + config['epochs'] - 1}\")\n",
    "    \n",
    "    for epoch in range(start_epoch, start_epoch + config[\"epochs\"]):\n",
    "        unet.train()\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch # Accelerator handles device\n",
    "            \n",
    "            # Encode images to latents\n",
    "            with torch.no_grad():\n",
    "                latents = vae.encode(clean_images).latent_dist.sample()\n",
    "                latents = latents * vae.config.scaling_factor\n",
    "\n",
    "            # Sample noise\n",
    "            noise = torch.randn_like(latents)\n",
    "            bs = latents.shape[0]\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bs,), device=latents.device).long()\n",
    "\n",
    "            # Add noise\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Predict noise\n",
    "            with accelerator.accumulate(unet):\n",
    "                noise_pred = unet(noisy_latents, timesteps).sample\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "            global_step += 1\n",
    "            \n",
    "            # Log loss to WandB\n",
    "            accelerator.log({\"train_loss\": loss.item()}, step=global_step)\n",
    "\n",
    "        # Save Checkpoint & Inference\n",
    "        if accelerator.is_main_process:\n",
    "            # Save every 5 epochs OR at the last epoch\n",
    "            if (epoch + 1) % 5 == 0 or epoch == start_epoch + config[\"epochs\"] - 1:\n",
    "                # Use unwrap_model to save cleanly\n",
    "                unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "                save_path = os.path.join(config[\"output_dir\"], f\"checkpoint-epoch-{epoch}\")\n",
    "                unwrapped_unet.save_pretrained(save_path)\n",
    "                print(f\"Saved model to {save_path}\")\n",
    "                \n",
    "            \n",
    "            # Inference visualization (every epoch for tracking)\n",
    "            # Use unwrap_model here too for inference\n",
    "            unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "            print(f\"Running inference visualization for epoch {epoch}...\")\n",
    "            vis_path = os.path.join(config[\"output_dir\"], f\"vis_epoch_{epoch}.png\")\n",
    "            simple_inference(unwrapped_unet, num_samples=4, save_path=vis_path)\n",
    "            \n",
    "            # Log image to WandB\n",
    "            accelerator.log({\"inference_samples\": wandb.Image(vis_path)}, step=global_step)\n",
    "\n",
    "    accelerator.end_training()\n",
    "    print(\"Training finished.\")\n",
    "    return unet # Returns trained model (wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb32e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAINING:\n",
    "    trained_unet = train_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d770f6d",
   "metadata": {},
   "source": [
    "## 4. Sequential Monte Carlo (SMC) Inference\n",
    "\n",
    "### Simple SMC inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b8e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smc_image_inference(image_path, target_emotion, num_particles=10, steps=50, guidance_scale=2.0, noise_strength=0.3, resample_interval=1, leash=0.0):\n",
    "    \"\"\"\n",
    "    Applies SMC inference to modify the emotion of an existing image.\n",
    "    Displays the progression of the best sample at each resampling step.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return\n",
    "\n",
    "    device = DEVICE\n",
    "    \n",
    "    # 1. Load Models\n",
    "    unet = UNet2DModel.from_pretrained(TEST_CONFIG[\"trained_unet_path\"])\n",
    "    unet.to(device)\n",
    "    unet.eval()\n",
    "    \n",
    "    vae = get_vae().to(device)\n",
    "    processor, classifier = get_emotion_classifier()\n",
    "    classifier.to(device)\n",
    "    \n",
    "    # Check emotion\n",
    "    id2label = classifier.config.id2label\n",
    "    label2id = {v.lower(): k for k, v in id2label.items()}\n",
    "    if target_emotion.lower() not in label2id:\n",
    "        print(f\"Emotion '{target_emotion}' not found. Available: {list(label2id.keys())}\")\n",
    "        return\n",
    "    target_class_id = label2id[target_emotion.lower()]\n",
    "    print(f\"Target: {target_emotion} (ID: {target_class_id})\")\n",
    "\n",
    "    # 2. Load & Preprocess Image\n",
    "    print(f\"Loading image from {image_path}...\")\n",
    "    transform = get_transforms(256)\n",
    "    original_pil = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = transform(original_pil).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 3. Encode to Latents\n",
    "    with torch.no_grad():\n",
    "        latents = vae.encode(image_tensor).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "        \n",
    "    # Duplicate latents for particles\n",
    "    latents = latents.repeat(num_particles, 1, 1, 1)\n",
    "    \n",
    "    # 4. Setup Scheduler\n",
    "    scheduler = DDPMScheduler(\n",
    "        num_train_timesteps=TRAIN_CONFIG[\"num_train_timesteps\"],\n",
    "        beta_schedule=TRAIN_CONFIG[\"beta_schedule\"],\n",
    "        prediction_type=TRAIN_CONFIG[\"prediction_type\"],\n",
    "        clip_sample=False\n",
    "    )\n",
    "    scheduler.set_timesteps(steps)\n",
    "    timesteps = scheduler.timesteps\n",
    "    \n",
    "    # 5. Add Noise (Start from t_start)\n",
    "    noise = torch.randn_like(latents)\n",
    "    start_idx = int((1.0 - noise_strength) * (len(timesteps) - 1))\n",
    "    t_start = timesteps[start_idx]\n",
    "    curr_latents = scheduler.add_noise(latents, noise, t_start)\n",
    "    \n",
    "    print(f\"Starting SMC Inference with {num_particles} particles, target='{target_emotion}'...\")\n",
    "    \n",
    "    # Prepare normalization constants from processor\n",
    "    # Default to ImageNet stats if not found\n",
    "    image_mean = getattr(processor, \"image_mean\", [0.485, 0.456, 0.406])\n",
    "    image_std = getattr(processor, \"image_std\", [0.229, 0.224, 0.225])\n",
    "    \n",
    "    norm_mean = torch.tensor(image_mean, device=device).view(1, 3, 1, 1)\n",
    "    norm_std = torch.tensor(image_std, device=device).view(1, 3, 1, 1)\n",
    "\n",
    "    progression_images = []\n",
    "\n",
    "    # 6. SMC Loop\n",
    "    for i, t in enumerate(tqdm(timesteps[start_idx:])):\n",
    "        # A. Denoise Step\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(curr_latents, t).sample\n",
    "            curr_latents = scheduler.step(noise_pred, t, curr_latents).prev_sample\n",
    "            \n",
    "            leash_t = leash * (1-i)/t_start\n",
    "            curr_latents = (1 - leash_t) * curr_latents + leash_t * latents\n",
    "            \n",
    "        # B. Resampling Step\n",
    "        # Do not resample at the very last step\n",
    "        if (i + 1) % resample_interval == 0 and (i + 1) < len(timesteps[start_idx:]):\n",
    "            with torch.no_grad():\n",
    "                # Decode for classification\n",
    "                temp_latents = curr_latents / vae.config.scaling_factor\n",
    "                decoded = vae.decode(temp_latents).sample\n",
    "                decoded = (decoded / 2 + 0.5).clamp(0, 1)\n",
    "                \n",
    "                # Resize for classifier (224x224)\n",
    "                images_resized = F.interpolate(decoded, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                \n",
    "                # Normalize for classifier using processor stats\n",
    "                classifier_inputs = (images_resized - norm_mean) / norm_std\n",
    "                \n",
    "                # Classify\n",
    "                outputs = classifier(pixel_values=classifier_inputs)\n",
    "                probs = F.softmax(outputs.logits, dim=-1)\n",
    "                \n",
    "                # Weighting\n",
    "                target_probs = probs[:, target_class_id]\n",
    "                \n",
    "                # --- Capture Best Particle ---\n",
    "                best_idx = torch.argmax(target_probs)\n",
    "                best_score = target_probs[best_idx].item()\n",
    "                best_img = decoded[best_idx].cpu().permute(1, 2, 0).numpy()\n",
    "                progression_images.append((f\"Step {i+1}\", best_img, best_score))\n",
    "                # -----------------------------\n",
    "\n",
    "                weights = target_probs ** guidance_scale\n",
    "                \n",
    "                if weights.sum() == 0:\n",
    "                    weights = torch.ones_like(weights) / len(weights)\n",
    "                else:\n",
    "                    weights = weights / weights.sum()\n",
    "                    \n",
    "                # Resample indices\n",
    "                indices = torch.multinomial(weights, num_samples=num_particles, replacement=True)\n",
    "                curr_latents = curr_latents[indices]\n",
    "                \n",
    "    # 7. Final Decode & Scoring\n",
    "    with torch.no_grad():\n",
    "        curr_latents = curr_latents / vae.config.scaling_factor\n",
    "        result_images_tensor = vae.decode(curr_latents).sample\n",
    "        result_images_tensor = (result_images_tensor / 2 + 0.5).clamp(0, 1)\n",
    "        \n",
    "        # Compute scores for final particles\n",
    "        images_resized = F.interpolate(result_images_tensor, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        classifier_inputs = (images_resized - norm_mean) / norm_std\n",
    "        \n",
    "        outputs = classifier(pixel_values=classifier_inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "        final_scores = probs[:, target_class_id].cpu().numpy()\n",
    "        \n",
    "        # Convert to numpy for display\n",
    "        result_images = result_images_tensor.cpu().permute(0, 2, 3, 1).numpy()\n",
    "        \n",
    "        # Add final best\n",
    "        best_final_idx = np.argmax(final_scores)\n",
    "        progression_images.append((\"Final\", result_images[best_final_idx], final_scores[best_final_idx]))\n",
    "        \n",
    "    # 8. Show Progression\n",
    "    num_images = len(progression_images) + 1\n",
    "    # Adjust figure size based on number of images\n",
    "    cols = min(num_images, 6)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    plt.figure(figsize=(3 * cols, 3.5 * rows))\n",
    "    \n",
    "    # Original\n",
    "    plt.subplot(rows, cols, 1)\n",
    "    plt.title(\"Original\")\n",
    "    plt.imshow(original_pil)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    for k, (label, img, score) in enumerate(progression_images):\n",
    "        plt.subplot(rows, cols, k + 2)\n",
    "        plt.title(f\"{label}\\nScore: {score:.4f}\")\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601abffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = TEST_CONFIG[\"test_image_path\"]\n",
    "smc_image_inference(test_image_path, target_emotion=\"sad\", num_particles=20, steps=250, guidance_scale=5.0, noise_strength=0.23, resample_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99768d",
   "metadata": {},
   "source": [
    "### Dual SMC inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d6f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smc_image_inference_dual(image_path, target_emotion1, target_emotion2, num_particles=10, steps=50, guidance_scale=2.0, noise_strength=0.3,  resample_interval=1, leash=0.0):\n",
    "    \"\"\"\n",
    "    Applies SMC inference for two target emotions and displays the original and both final results.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return\n",
    "\n",
    "    device = DEVICE\n",
    "    \n",
    "    # 1. Load Models\n",
    "    unet = UNet2DModel.from_pretrained(TEST_CONFIG[\"trained_unet_path\"])\n",
    "    unet.to(device)\n",
    "    unet.eval()\n",
    "    \n",
    "    vae = get_vae().to(device)\n",
    "    processor, classifier = get_emotion_classifier()\n",
    "    classifier.to(device)\n",
    "    \n",
    "    # Check emotions\n",
    "    id2label = classifier.config.id2label\n",
    "    label2id = {v.lower(): k for k, v in id2label.items()}\n",
    "    \n",
    "    targets = []\n",
    "    for emo in [target_emotion1, target_emotion2]:\n",
    "        if emo.lower() not in label2id:\n",
    "            print(f\"Emotion '{emo}' not found. Available: {list(label2id.keys())}\")\n",
    "            return\n",
    "        targets.append((emo, label2id[emo.lower()]))\n",
    "\n",
    "    # 2. Load & Preprocess Image (Common)\n",
    "    print(f\"Loading image from {image_path}...\")\n",
    "    transform = get_transforms(256)\n",
    "    original_pil = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = transform(original_pil).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Normalization constants\n",
    "    image_mean = getattr(processor, \"image_mean\", [0.485, 0.456, 0.406])\n",
    "    image_std = getattr(processor, \"image_std\", [0.229, 0.224, 0.225])\n",
    "    norm_mean = torch.tensor(image_mean, device=device).view(1, 3, 1, 1)\n",
    "    norm_std = torch.tensor(image_std, device=device).view(1, 3, 1, 1)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for target_emotion, target_class_id in targets:\n",
    "        print(f\"Starting SMC Inference for target='{target_emotion}'...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            latents = vae.encode(image_tensor).latent_dist.sample()\n",
    "            latents = latents * vae.config.scaling_factor\n",
    "            \n",
    "        # Duplicate latents for particles\n",
    "        latents = latents.repeat(num_particles, 1, 1, 1)\n",
    "        \n",
    "        # 4. Setup Scheduler\n",
    "        scheduler = DDPMScheduler(\n",
    "            num_train_timesteps=TRAIN_CONFIG[\"num_train_timesteps\"],\n",
    "            beta_schedule=TRAIN_CONFIG[\"beta_schedule\"],\n",
    "            prediction_type=TRAIN_CONFIG[\"prediction_type\"],\n",
    "            clip_sample=False\n",
    "        )\n",
    "        scheduler.set_timesteps(steps)\n",
    "        timesteps = scheduler.timesteps\n",
    "        \n",
    "        # 5. Add Noise\n",
    "        noise = torch.randn_like(latents)\n",
    "        start_idx = int((1.0 - noise_strength) * (len(timesteps) - 1))\n",
    "        t_start = timesteps[start_idx]\n",
    "        curr_latents = scheduler.add_noise(latents, noise, t_start)\n",
    "        \n",
    "        # 6. SMC Loop\n",
    "        for i, t in enumerate(tqdm(timesteps[start_idx:], desc=f\"SMC {target_emotion}\")):\n",
    "            # A. Denoise Step\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(curr_latents, t).sample\n",
    "                curr_latents = scheduler.step(noise_pred, t, curr_latents).prev_sample\n",
    "                \n",
    "                leash_t = leash * (1-i)/t_start\n",
    "                curr_latents = (1 - leash_t) * curr_latents + leash_t * latents\n",
    "                \n",
    "            # B. Resampling Step\n",
    "            if (i + 1) % resample_interval == 0 and (i + 1) < len(timesteps[start_idx:]):\n",
    "                with torch.no_grad():\n",
    "                    temp_latents = curr_latents / vae.config.scaling_factor\n",
    "                    decoded = vae.decode(temp_latents).sample\n",
    "                    decoded = (decoded / 2 + 0.5).clamp(0, 1)\n",
    "                    images_resized = F.interpolate(decoded, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                    classifier_inputs = (images_resized - norm_mean) / norm_std\n",
    "                    outputs = classifier(pixel_values=classifier_inputs)\n",
    "                    probs = F.softmax(outputs.logits, dim=-1)\n",
    "                    target_probs = probs[:, target_class_id]\n",
    "                    \n",
    "                    weights = target_probs ** guidance_scale\n",
    "                    if weights.sum() == 0:\n",
    "                        weights = torch.ones_like(weights) / len(weights)\n",
    "                    else:\n",
    "                        weights = weights / weights.sum()\n",
    "                        \n",
    "                    indices = torch.multinomial(weights, num_samples=num_particles, replacement=True)\n",
    "                    curr_latents = curr_latents[indices]\n",
    "        \n",
    "        # 7. Final Decode & Scoring\n",
    "        with torch.no_grad():\n",
    "            curr_latents = curr_latents / vae.config.scaling_factor\n",
    "            result_images_tensor = vae.decode(curr_latents).sample\n",
    "            result_images_tensor = (result_images_tensor / 2 + 0.5).clamp(0, 1)\n",
    "            \n",
    "            images_resized = F.interpolate(result_images_tensor, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            classifier_inputs = (images_resized - norm_mean) / norm_std\n",
    "            \n",
    "            outputs = classifier(pixel_values=classifier_inputs)\n",
    "            probs = F.softmax(outputs.logits, dim=-1)\n",
    "            final_scores = probs[:, target_class_id].cpu().numpy()\n",
    "            \n",
    "            result_images = result_images_tensor.cpu().permute(0, 2, 3, 1).numpy()\n",
    "            \n",
    "            best_final_idx = np.argmax(final_scores)\n",
    "            results.append((target_emotion, result_images[best_final_idx], final_scores[best_final_idx]))\n",
    "\n",
    "    # 8. Save and Provide Link\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Original\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Original\")\n",
    "    plt.imshow(original_pil)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Result 1\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"{results[0][0]}\\nScore: {results[0][2]:.4f}\")\n",
    "    plt.imshow(results[0][1])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Result 2\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(f\"{results[1][0]}\\nScore: {results[1][2]:.4f}\")\n",
    "    plt.imshow(results[1][1])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_filename = \"dual_inference_result.png\"\n",
    "    plt.savefig(output_filename)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Image saved to: {output_filename}\")\n",
    "    from IPython.display import FileLink, display\n",
    "    display(FileLink(output_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a59c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = TEST_CONFIG[\"test_image_path\"]\n",
    "smc_image_inference_dual(test_image_path, 'happy', 'sad', num_particles=20, steps=250, guidance_scale=2.0, noise_strength=0.23, resample_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc66f27",
   "metadata": {},
   "source": [
    "## 5. Benchmarking & Metrics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12354ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q facenet-pytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from facenet_pytorch import MTCNN\n",
    "import random\n",
    "\n",
    "def smc_inference_core(image_path, unet, vae, classifier, processor, target_emotion, num_particles, steps, guidance_scale, noise_strength, leash, resample_interval):\n",
    "    device = unet.device\n",
    "    \n",
    "    # Check emotion\n",
    "    id2label = classifier.config.id2label\n",
    "    label2id = {v.lower(): k for k, v in id2label.items()}\n",
    "    if target_emotion.lower() not in label2id:\n",
    "        return None, 0.0\n",
    "    target_class_id = label2id[target_emotion.lower()]\n",
    "\n",
    "    # Load & Preprocess\n",
    "    transform = get_transforms(256)\n",
    "    original_pil = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = transform(original_pil).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Encode\n",
    "    with torch.no_grad():\n",
    "        latents = vae.encode(image_tensor).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "        \n",
    "    latents = latents.repeat(num_particles, 1, 1, 1)\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler = DDPMScheduler(\n",
    "        num_train_timesteps=TRAIN_CONFIG[\"num_train_timesteps\"],\n",
    "        beta_schedule=TRAIN_CONFIG[\"beta_schedule\"],\n",
    "        prediction_type=TRAIN_CONFIG[\"prediction_type\"],\n",
    "        clip_sample=False\n",
    "    )\n",
    "    scheduler.set_timesteps(steps)\n",
    "    timesteps = scheduler.timesteps\n",
    "    \n",
    "    # Add Noise\n",
    "    noise = torch.randn_like(latents)\n",
    "    start_idx = int((1.0 - noise_strength) * (len(timesteps) - 1))\n",
    "    t_start = timesteps[start_idx]\n",
    "    curr_latents = scheduler.add_noise(latents, noise, t_start)\n",
    "    \n",
    "    # Normalization constants\n",
    "    image_mean = getattr(processor, \"image_mean\", [0.485, 0.456, 0.406])\n",
    "    image_std = getattr(processor, \"image_std\", [0.229, 0.224, 0.225])\n",
    "    norm_mean = torch.tensor(image_mean, device=device).view(1, 3, 1, 1)\n",
    "    norm_std = torch.tensor(image_std, device=device).view(1, 3, 1, 1)\n",
    "\n",
    "    # SMC Loop\n",
    "    for i, t in enumerate(timesteps[start_idx:]):\n",
    "        # Denoise\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(curr_latents, t).sample\n",
    "            curr_latents = scheduler.step(noise_pred, t, curr_latents).prev_sample\n",
    "            \n",
    "            leash_t = leash * (1-i)/t_start\n",
    "            curr_latents = (1 - leash_t) * curr_latents + leash_t * latents\n",
    "            \n",
    "        # Resample\n",
    "        if (i + 1) % resample_interval == 0 and (i + 1) < len(timesteps[start_idx:]):\n",
    "            with torch.no_grad():\n",
    "                temp_latents = curr_latents / vae.config.scaling_factor\n",
    "                decoded = vae.decode(temp_latents).sample\n",
    "                decoded = (decoded / 2 + 0.5).clamp(0, 1)\n",
    "                images_resized = F.interpolate(decoded, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                classifier_inputs = (images_resized - norm_mean) / norm_std\n",
    "                outputs = classifier(pixel_values=classifier_inputs)\n",
    "                probs = F.softmax(outputs.logits, dim=-1)\n",
    "                target_probs = probs[:, target_class_id]\n",
    "                \n",
    "                weights = target_probs ** guidance_scale\n",
    "                if weights.sum() == 0:\n",
    "                    weights = torch.ones_like(weights) / len(weights)\n",
    "                else:\n",
    "                    weights = weights / weights.sum()\n",
    "                    \n",
    "                indices = torch.multinomial(weights, num_samples=num_particles, replacement=True)\n",
    "                curr_latents = curr_latents[indices]\n",
    "                \n",
    "    # Final Decode\n",
    "    with torch.no_grad():\n",
    "        curr_latents = curr_latents / vae.config.scaling_factor\n",
    "        result_images_tensor = vae.decode(curr_latents).sample\n",
    "        result_images_tensor = (result_images_tensor / 2 + 0.5).clamp(0, 1)\n",
    "        \n",
    "        images_resized = F.interpolate(result_images_tensor, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        classifier_inputs = (images_resized - norm_mean) / norm_std\n",
    "        outputs = classifier(pixel_values=classifier_inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "        final_scores = probs[:, target_class_id].cpu().numpy()\n",
    "        \n",
    "        best_idx = np.argmax(final_scores)\n",
    "        best_score = final_scores[best_idx]\n",
    "        best_image_tensor = result_images_tensor[best_idx] # [3, H, W]\n",
    "        \n",
    "    return best_image_tensor, best_score\n",
    "\n",
    "def run_benchmark_tests(tests, images_per_test=10):\n",
    "    print(\"Initializing benchmark...\")\n",
    "    \n",
    "    # 1. Setup Data\n",
    "    base_dir = TRAIN_CONFIG[\"data_dir\"]\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"⚠️ Path {base_dir} not found. Check dataset.\")\n",
    "        image_paths = []\n",
    "    else:\n",
    "        # Select first N images\n",
    "        all_files = sorted(os.listdir(base_dir))\n",
    "        image_paths = []\n",
    "        for filename in all_files:\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_paths.append(os.path.join(base_dir, filename))\n",
    "            if len(image_paths) >= images_per_test:\n",
    "                break\n",
    "\n",
    "    # 2. Setup Models\n",
    "    device = DEVICE\n",
    "    resume_from_checkpoint = TRAIN_CONFIG[\"checkpoint_path\"]\n",
    "    \n",
    "    print(\"Loading models...\")\n",
    "    if resume_from_checkpoint and os.path.exists(resume_from_checkpoint):\n",
    "        unet = UNet2DModel.from_pretrained(resume_from_checkpoint)\n",
    "    else:\n",
    "        print(f\"Checkpoint {resume_from_checkpoint} not found.\")\n",
    "        return()\n",
    "    \n",
    "    unet.to(device)\n",
    "    unet.eval()\n",
    "    \n",
    "    vae = get_vae().to(device)\n",
    "    processor, classifier = get_emotion_classifier()\n",
    "    classifier.to(device)\n",
    "    \n",
    "    # Initialize MTCNN\n",
    "    print(\"Initializing MTCNN...\")\n",
    "    mtcnn = MTCNN(keep_all=True, device=device)\n",
    "    \n",
    "    \n",
    "    results_log = []\n",
    "\n",
    "    for test in tests:\n",
    "        print(f\"\\n=== Running {test['name']} ===\")\n",
    "        for val in test['values']:\n",
    "            params = test['fixed'].copy()\n",
    "            params[test['var']] = val\n",
    "            \n",
    "            print(f\"Testing {test['var']} = {val} ...\")\n",
    "            \n",
    "            scores = []\n",
    "            face_probs = []\n",
    "            \n",
    "            for img_path in tqdm(image_paths, leave=False):\n",
    "                # Run Inference\n",
    "                target_emotion = random.choice([\"happy\", \"sad\", \"angry\", \"surprise\", \"neutral\", \"fear\", \"disgust\"])\n",
    "                final_img_tensor, score = smc_inference_core(\n",
    "                    img_path, unet, vae, classifier, processor, target_emotion, \n",
    "                    num_particles=params.get(\"particles\"),\n",
    "                    steps=params.get(\"steps\"),\n",
    "                    guidance_scale=params.get(\"guidance_scale\"),\n",
    "                    noise_strength=params.get(\"noise_strength\"),\n",
    "                    leash=0.0,\n",
    "                    resample_interval=1\n",
    "                )\n",
    "                \n",
    "                scores.append(score)\n",
    "                \n",
    "                # MTCNN Detection\n",
    "                try:\n",
    "                    # Convert tensor [3, H, W] to PIL\n",
    "                    img_np = final_img_tensor.cpu().permute(1, 2, 0).numpy()\n",
    "                    img_pil = Image.fromarray((img_np * 255).astype(np.uint8))\n",
    "                    \n",
    "                    _, probs = mtcnn.detect(img_pil)\n",
    "                    \n",
    "                    if probs is not None and len(probs) > 0:\n",
    "                        # Take max probability (but there should be one element in probs, since there is only one face)\n",
    "                        face_probs.append(np.max(probs))\n",
    "                    else:\n",
    "                        face_probs.append(0.0)\n",
    "                except Exception as e:\n",
    "                    print(f\"MTCNN Error: {e}\")\n",
    "                    face_probs.append(0.0)\n",
    "                    \n",
    "                # print(\"score:\", score, \"face_prob:\", face_probs[-1])\n",
    "                \n",
    "            avg_score = np.mean(scores) if scores else 0.0\n",
    "            avg_face_prob = np.mean(face_probs) if face_probs else 0.0\n",
    "            \n",
    "            print(f\"-> Result: Avg Emotion Score={avg_score:.4f}, Avg Face Prob={avg_face_prob:.4f}\")\n",
    "            \n",
    "            entry = params.copy()\n",
    "            entry[\"tested_var\"] = test['var']\n",
    "            entry[\"tested_value\"] = val\n",
    "            entry[\"Avg_Emotion_Score\"] = avg_score\n",
    "            entry[\"Avg_Face_Prob\"] = avg_face_prob\n",
    "            results_log.append(entry)\n",
    "            \n",
    "    # Display Final Table\n",
    "    df_results = pd.DataFrame(results_log)\n",
    "    print(\"\\n=== Complete Results ===\")\n",
    "    display(df_results)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fbf828",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "        {\"name\": \"Guidance Scale Test\", \"fixed\": {\"particles\": 20, \"noise_strength\": 0.2, \"steps\": 250}, \"var\": \"guidance_scale\", \"values\": [1, 2, 5, 10]},\n",
    "        {\"name\": \"Noise Strength Test\", \"fixed\": {\"particles\": 20, \"steps\": 250, \"guidance_scale\": 5}, \"var\": \"noise_strength\", \"values\": [0.1, 0.2, 0.3, 0.7]},\n",
    "        {\"name\": \"Steps Test\", \"fixed\": {\"particles\": 20, \"noise_strength\": 0.2, \"guidance_scale\": 5}, \"var\": \"steps\", \"values\": [50, 150, 250, 500]},\n",
    "]\n",
    "\n",
    "if RUN_BENCHMARKING:\n",
    "    df_benchmark = run_benchmark_tests(tests, images_per_test=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
